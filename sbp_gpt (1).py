# -*- coding: utf-8 -*-
"""SBP_GPT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/AmmarJamshed/Course_training_Natural_language_processing/blob/main/SBP_GPT.ipynb
"""

import requests
from bs4 import BeautifulSoup

# URL of the webpage to scrape
url = "https://www.sbp.org.pk/l_frame/index2.asp"

# Send a request to fetch the webpage content
response = requests.get(url)
web_content = response.content

# Parse the webpage content
soup = BeautifulSoup(web_content, 'html.parser')

# Find all links to PDF files
base_url = "https://www.sbp.org.pk"
pdf_links = []
for a in soup.find_all('a', href=True):
    href = a['href']
    if href.endswith('.pdf'):
        if href.startswith('http'):
            full_url = href
        else:
            full_url = base_url + href if href.startswith('/') else base_url + '/' + href
        pdf_links.append(full_url)

# Display the PDF links
for link in pdf_links:
    print(link)

import os

# Directory to save the downloaded PDFs
os.makedirs('pdfs', exist_ok=True)

# Function to download PDFs
def download_pdf(url, directory):
    try:
        local_filename = os.path.join(directory, url.split('/')[-1])
        with requests.get(url, stream=True) as r:
            r.raise_for_status()
            with open(local_filename, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
        return local_filename
    except requests.exceptions.HTTPError as http_err:
        print(f"HTTP error occurred: {http_err} - {url}")
    except Exception as err:
        print(f"Other error occurred: {err} - {url}")

# Download each PDF and handle errors
for link in pdf_links:
    print(f"Downloading {link}")
    download_pdf(link, 'pdfs')

pip install PyPDF2

import os
import PyPDF2

# Function to extract text from PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as f:
        reader = PyPDF2.PdfReader(f)
        for page_num in range(len(reader.pages)):
            page = reader.pages[page_num]
            text += page.extract_text()
    return text

# Extract text from all downloaded PDFs
all_text = ""
pdf_files = [os.path.join('pdfs', f) for f in os.listdir('pdfs') if f.endswith('.pdf')]
for pdf_file in pdf_files:
    all_text += extract_text_from_pdf(pdf_file)

# Save extracted text to a file
with open('audit_regulations_text.txt', 'w', encoding='utf-8') as f:
    f.write(all_text)

import re

# Read the extracted text
with open('audit_regulations_text.txt', 'r', encoding='utf-8') as f:
    text = f.read()

# Clean the text
def clean_text(text):
    text = re.sub(r'\s+', ' ', text)  # Remove extra whitespace
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)  # Remove special characters
    return text

cleaned_text = clean_text(text)

# Save cleaned text
with open('cleaned_audit_regulations_text.txt', 'w', encoding='utf-8') as f:
    f.write(cleaned_text)

import csv

# Path to the input text file and the output CSV file
input_text_file = '/content/cleaned_audit_regulations_text.txt'
output_csv_file = '/content/cleaned_audit_regulations_text.csv'

# Read the text file
with open(input_text_file, 'r', encoding='utf-8') as f:
    lines = f.readlines()

# Write to CSV file
with open(output_csv_file, 'w', newline='', encoding='utf-8') as csvfile:
    writer = csv.writer(csvfile)

    # Optionally, write a header row
    writer.writerow(['text'])

    # Write each line to a new row in the CSV file
    for line in lines:
        writer.writerow([line.strip()])

print(f"Data successfully written to {output_csv_file}")

pip install accelerate -U

import pandas as pd
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments

# Load tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

# Configure the tokenizer to handle long sequences
tokenizer.model_max_length = 1024
tokenizer.padding_side = "right"
tokenizer.truncation_side = "right"

# Create a dataset object
class AuditRegulationsDataset(torch.utils.data.Dataset):
    def __init__(self, data, tokenizer, max_length=1024):
        self.examples = []
        self.max_length = max_length
        for text in data:
            # Encode and truncate the text
            encoded_text = tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=max_length)
            self.examples.append(encoded_text)

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, i):
        return torch.tensor(self.examples[i], dtype=torch.long)

train_data = pd.read_csv('/content/cleaned_audit_regulations_text.csv')['text'].tolist()
dataset = AuditRegulationsDataset(train_data, tokenizer)

# Data collator
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# Training arguments
training_args = TrainingArguments(
    output_dir='./results',
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=4,
    save_steps=10_000,
    save_total_limit=2,
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
)

# Train the model
trainer.train()

import pandas as pd

# Define the achieved metrics
achieved_metrics = {
    'global_step': 20,
    'training_loss': 2.9112009048461913,
    'train_runtime': 532.0636,
    'train_samples_per_second': 0.038,
    'train_steps_per_second': 0.038,
    'total_flos': 10451681280000.0,
    'epoch': 20.0
}

# Define the benchmarked metrics
benchmark_metrics = {
    'global_step': 'Higher is better',
    'training_loss': '< 1.0 is good',
    'train_runtime': 'Depends on hardware, shorter is better',
    'train_samples_per_second': '> 10 for GPUs, > 1 for CPUs',
    'train_steps_per_second': '> 1 is good',
    'total_flos': 'Depends on model complexity, higher is usual',
    'epoch': '> 10 is generally good'
}

# Create a DataFrame for comparison
comparison_df = pd.DataFrame({
    'Metric': achieved_metrics.keys(),
    'Achieved': achieved_metrics.values(),
    'Benchmark': benchmark_metrics.values()
})

# Calculate and interpret the comparison
comparison_df['Evaluation'] = [
    'Needs Improvement' if metric == 'training_loss' and achieved > 1.0 else
    'Good' if metric == 'training_loss' and achieved <= 1.0 else
    'Acceptable' if metric == 'train_runtime' else
    'Good' if metric in ['train_samples_per_second', 'train_steps_per_second'] and achieved > 1.0 else
    'Needs Improvement' if metric in ['train_samples_per_second', 'train_steps_per_second'] and achieved <= 1.0 else
    'Good' if metric == 'epoch' and achieved >= 10 else
    'Needs Improvement' if metric == 'epoch' and achieved < 10 else
    'N/A'
    for metric, achieved in zip(achieved_metrics.keys(), achieved_metrics.values())
]

# Print the comparison table
print(comparison_df)

# Save the comparison table to a CSV file
output_csv_file = 'model_metrics_comparison.csv'
comparison_df.to_csv(output_csv_file, index=False, encoding='utf-8')
comparison_df

# Save the trained model
trainer.save_model('./results')

import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel

# Load the tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('./results')  # Adjust this path if needed

# Function to generate text based on a prompt
def generate_text(prompt, model, tokenizer, max_length=50, num_return_sequences=1):
    # Encode the prompt
    input_ids = tokenizer.encode(prompt, return_tensors='pt')

    # Generate text
    output = model.generate(
        input_ids,
        max_length=max_length,
        num_return_sequences=num_return_sequences,
        no_repeat_ngram_size=2,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        temperature=1.0
    )

    # Decode the generated text
    generated_text = [tokenizer.decode(output_seq, skip_special_tokens=True) for output_seq in output]
    return generated_text

# Interactive prompt input
prompt = input("Enter a prompt: ")

# Generate the text
generated_text = generate_text(prompt, model, tokenizer)

# Save generated text to a file
output_file = 'generated_text_results.txt'
with open(output_file, 'w', encoding='utf-8') as f:
    for i, text in enumerate(generated_text):
        f.write(f"Generated Text {i+1}:\n{text}\n\n")

print(f"Generated text has been saved to {output_file}")

# Print the generated text
for i, text in enumerate(generated_text):
    print(f"Generated Text {i+1}:\n{text}\n")